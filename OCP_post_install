##################################################
1. Update NTP and timezone
##################################################

for i in `oc get nodes | grep -iv name | awk '{print $1}'`; do ssh core@$i "sudo timedatectl set-timezone Asia/Bangkok"; done 
for i in `oc get nodes | grep -iv name | awk '{print $1}'`; do echo $i; ssh core@$i "sudo timedatectl"; done 

cat /etc/chrony.conf | python3 -c "import sys, urllib.parse; print(urllib.parse.quote(''.join(sys.stdin.readlines())))"

vi 50-master-chrony
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 50-master-chrony
spec:
  config:
    ignition:
      version: 2.2.0
    storage:
      files:
      - contents:
          source: data:,%23%20Use%20public%23%20Enable%20kernel%20synchronization%20
        filesystem: root
        mode: 0644
        path: /etc/chrony.conf

vi 50-worker-chrony
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 50-worker-chrony
spec:
  config:
    ignition:
      version: 2.2.0
    storage:
      files:
      - contents:
          source: data:,%23%20Use%20public%20servers%20from%20the%20pool.ntp.org%20
        filesystem: root
        mode: 0644
        path: /etc/chrony.conf

oc create -f 50-master-chrony
oc create -f 50-worker-chrony
watch oc get mcp
for i in `oc get nodes | grep -iv name | awk '{print $1}'`; do echo $i; ssh core@$i "sudo chronyc sources"; done 

##################################################
2.Create node label
##################################################

oc label node <node> node-role.kubernetes.io/infra-router=""
oc label node <node> node-role.kubernetes.io/infra-monitoring=""

oc label node <node> node-role.kubernetes.io/worker-

oc get nodes --show-labels

##################################################
3. Create infra machineconfigpool
##################################################

cat <<EOF | oc apply -f -
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata: 
  name: infra
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: 
[worker,infra]}
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/infra-router: ""
  paused: false
EOF


cat <<EOF | oc apply -f -
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata: 
  name: monitoring
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: 
[worker,monitoring]}
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/infra-monitoring: ""
  paused: false
EOF

oc get mcp

##################################################
4. Config ImageRegistry Operator to managed state
##################################################

oc edit configs.imageregistry.operator.openshift.io

managementState: Removed -> Managed

  storage:
    emptyDir: {}

oc patch configs.imageregistry.operator.openshift.io/cluster --patch '{"spec":{"defaultRoute":true}}' --type=merge

##################################################
5. taint node and torelation
##################################################

oc adm taint node <Infra Node1> <Infra Node1> infra-router=reserved:NoSchedule
oc adm taint node <Monitoring Node1> <Monitoring Node2> <Monitoring Node3> infra-monitoring=reserved:NoSchedule

##################################################
6. Move OCP Services to Infra and Monitoring nodes
##################################################

move router pods

oc patch ingresscontroller/default --type=merge -n openshift-ingress-operator -p '{"spec": {"nodePlacement":{"nodeSelector":{"matchLabels":{"node-role.kubernetes.io/infra-router": ""}},"tolerations": [{"effect":"NoSchedule","key": "infra-router","value": "reserved"}]}}}'
watch "oc -n openshift-ingress get pod -o wide"

move image-registry pod

oc -n openshift-image-registry patch configs.imageregistry.operator.openshift.io cluster --type=merge --patch='{"spec":{"nodeSelector": {"node-role.kubernetes.io/infra-monitoring":""},"tolerations": [{"effect":"NoSchedule","key": "infra-monitoring","value": "reserved"}]}}'
watch "oc -n openshift-image-registry get pod -o wide | grep '^image-registry-'"

##################################################
7. Installing the Local Storage Operator
##################################################

To install the Local Storage Operator from the web console, follow these steps:

    - Log in to the OpenShift Container Platform web console.
    - Navigate to Operators → OperatorHub.
    - Type Local Storage into the filter box to locate the Local Storage Operator.
    - Click Install.
    - On the Create Operator Subscription page, select A specific namespace on the cluster. Select local-storage from the drop-down menu.
    - Adjust the values for Update Channel and Approval Strategy to the values that you want.
    - Click Subscribe.

Once finished, the Local Storage Operator will be listed in the Installed Operators section of the web console.

oc new-project local-storage

oc patch ds local-storage-local-diskmaker -n local-storage -p '{"spec": {"template": {"spec": {"tolerations":[{"operator": "Exists"}]}}}}'
oc patch ds local-storage-local-provisioner -n local-storage -p '{"spec": {"template": {"spec": {"tolerations":[{"operator": "Exists"}]}}}}'

oc get csvs -n local-storage

On Monitoring node1

vgcreate monitoring_vg /dev/sdb
lvcreate -n elasticsearch1_lv -L 300G monitoring_vg
lvcreate -n prometheus1_lv -L 200G monitoring_vg
lvcreate -n alertmanager1_lv -L 5G monitoring_vg

On Monitoring node2

vgcreate monitoring_vg /dev/sdb
lvcreate -n elasticsearch1_lv -L 300G monitoring_vg
lvcreate -n prometheus1_lv -L 200G monitoring_vg
lvcreate -n alertmanager1_lv -L 5G monitoring_vg

On Monitoring node3

vgcreate monitoring_vg /dev/sdb
lvcreate -n elasticsearch1_lv -L 300G monitoring_vg
lvcreate -n alertmanager1_lv -L 5G monitoring_vg


vi local-volume-elasticsearch.yml

apiVersion: "local.storage.openshift.io/v1"
kind: "LocalVolume"
metadata:
  name: "local-storage-elasticsearch"
  namespace: "local-storage"
spec:
  nodeSelector:
    nodeSelectorTerms:
    - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - <Strorage Node1>
          - <Strorage Node2>
          - <Strorage Node3>
  tolerations:
  - operator: Exists
  storageClassDevices:
    - storageClassName: "local-sc-elasticsearch"
      volumeMode: Filesystem
      fsType: xfs
      devicePaths:
        - /dev/monitoring_vg/elasticsearch1_lv

vi local-volume-alertmanager.yml

apiVersion: "local.storage.openshift.io/v1"
kind: "LocalVolume"
metadata:
  name: "local-storage-alertmanager"
  namespace: "local-storage" 
spec:
  nodeSelector: 
    nodeSelectorTerms:
    - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - <Strorage Node1>
          - <Strorage Node2>
          - <Strorage Node3>
  tolerations:
  - operator: Exists
  storageClassDevices:
    - storageClassName: "local-sc-alertmanager"
      volumeMode: Filesystem
      fsType: xfs
      devicePaths: 
        - /dev/monitoring_vg/alertmanager1_lv

vi local-volume-prometheus.yml

apiVersion: "local.storage.openshift.io/v1"
kind: "LocalVolume"
metadata:
  name: "local-storage-prometheus"
  namespace: "local-storage" 
spec:
  nodeSelector: 
    nodeSelectorTerms:
    - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - <Strorage Node1>
          - <Strorage Node2>
  tolerations:
  - operator: Exists
  storageClassDevices:
    - storageClassName: "local-sc-prometheus"
      volumeMode: Filesystem
      fsType: xfs
      devicePaths: 
        - /dev/monitoring_vg/prometheus1_lv

oc create -f local-volume-elasticsearch.yml
oc create -f local-volume-alertmanager.yml
oc create -f local-volume-prometheus.yml

##################################################
8. Deploy monitoring stack
##################################################

vi monitoring_stack.yml

apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusOperator:
      nodeSelector:
        node-role.kubernetes.io/infra-monitoring: ""
      tolerations:
      - key: infra-monitoring
        value: reserved
        effect: NoSchedule
    prometheusK8s:
      retention: 15d
      nodeSelector:
        node-role.kubernetes.io/infra-monitoring: ""
      tolerations:
      - key: infra-monitoring
        value: reserved
        effect: NoSchedule
      volumeClaimTemplate:
        metadata:
          name: local-pvc-prometheus
        spec:
          storageClassName: local-sc-prometheus
          resources:
            requests:
              storage: 200Gi
    alertmanagerMain:
      nodeSelector:
        node-role.kubernetes.io/infra-monitoring: ""
      tolerations:
      - key: infra-monitoring
        value: reserved
        effect: NoSchedule
      volumeClaimTemplate:
        metadata:
          name: local-pvc-alertmanager
        spec:
          storageClassName: local-sc-alertmanager
          resources:
            requests:
              storage: 5Gi
    kubeStateMetrics:
      nodeSelector:
        node-role.kubernetes.io/infra-monitoring: ""
      tolerations:
      - key: infra-monitoring
        value: reserved
        effect: NoSchedule
    grafana:
      nodeSelector:
        node-role.kubernetes.io/infra-monitoring: ""
      tolerations:
      - key: infra-monitoring
        value: reserved
        effect: NoSchedule
    telemeterClient:
      nodeSelector:
        node-role.kubernetes.io/infra-monitoring: ""
      tolerations:
      - key: infra-monitoring
        value: reserved
        effect: NoSchedule
    k8sPrometheusAdapter:
      nodeSelector:
        node-role.kubernetes.io/infra-monitoring: ""
      tolerations:
      - key: infra-monitoring
        value: reserved
        effect: NoSchedule
    openshiftStateMetrics:
      nodeSelector:
        node-role.kubernetes.io/infra-monitoring: ""
      tolerations:
      - key: infra-monitoring
        value: reserved
        effect: NoSchedule
    thanosQuerier:
      nodeSelector:
        node-role.kubernetes.io/infra-monitoring: ""
      tolerations:
      - key: infra-monitoring
        value: reserved
        effect: NoSchedule

oc apply -f monitoring_stack.yml

##################################################
9. install the Elasticsearch Operator and Cluster Logging Operator
##################################################

vi eo-namespace.yaml

apiVersion: v1
kind: Namespace
metadata:
  name: openshift-operators-redhat 
  annotations:
    openshift.io/node-selector: ""
  labels:
    openshift.io/cluster-monitoring: "true"

vi clo-namespace.yaml

apiVersion: v1
kind: Namespace
metadata:
  name: openshift-logging
  annotations:
    openshift.io/node-selector: ""
  labels:
    openshift.io/cluster-monitoring: "true"

vi eo-og.yaml

apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: openshift-operators-redhat
  namespace: openshift-operators-redhat 
spec: {}

vi eo-sub.yaml

apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: "elasticsearch-operator"
  namespace: "openshift-operators-redhat" 
spec:
  channel: "4.5" 
  installPlanApproval: "Automatic"
  source: "redhat-operators" 
  sourceNamespace: "openshift-marketplace"
  name: "elasticsearch-operator"


oc create -f eo-namespace.yaml
oc create -f clo-namespace.yaml
oc create -f eo-og.yaml
oc create -f eo-sub.yaml
oc get csv --all-namespaces


vi clo-og.yaml

apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: cluster-logging
  namespace: openshift-logging 
spec:
  targetNamespaces:
  - openshift-logging

vi clo-sub.yaml

apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: cluster-logging
  namespace: openshift-logging 
spec:
  channel: "4.5" 
  name: cluster-logging
  source: redhat-operators 
  sourceNamespace: openshift-marketplace


oc create -f clo-og.yaml
oc create -f clo-sub.yaml
oc get csv -n openshift-logging


vi clo-instance.yaml

apiVersion: "logging.openshift.io/v1"
kind: "ClusterLogging"
metadata:
#  annotations:
#    clusterlogging.openshift.io/logforwardingtechpreview: enabled
  name: "instance" 
  namespace: "openshift-logging"
spec:
  managementState: "Managed"  
  logStore:
    type: "elasticsearch"  
    retentionPolicy: 
      application:
        maxAge: 7d
      infra:
        maxAge: 7d
      audit:
        maxAge: 7d
    elasticsearch:
      nodeCount: 3
      nodeSelector:
        node-role.kubernetes.io/infra-monitoring: ""
      storage:
        storageClassName: "local-sc-elasticsearch" 
        size: 195G
      redundancyPolicy: "SingleRedundancy"
      resources:
        limits:
          cpu: 3
          memory: 8Gi
        requests:
          cpu: 3
          memory: 8Gi  
      tolerations:
      - key: infra-monitoring
        value: reserved
        effect: NoSchedule
  visualization:
    type: "kibana"  
    kibana:
      replicas: 1
      nodeSelector:
        node-role.kubernetes.io/infra-monitoring: ""
      tolerations:
      - key: infra-monitoring
        value: reserved
        effect: NoSchedule
  curation:
    type: "curator"
    curator:
      schedule: "30 3 * * *" 
      nodeSelector:
        node-role.kubernetes.io/infra-monitoring: ""
      tolerations:
      - key: infra-monitoring
        value: reserved
        effect: NoSchedule
  collection:
    logs:
      type: "fluentd"  
      fluentd:
        tolerations:
        - effect: NoSchedule
          key: infra-monitoring
          value: reserved
        - effect: NoSchedule
          key: infra-router
          value: reserved

oc create -f clo-instance.yaml
oc adm policy add-cluster-role-to-user cluster-admin kube:admin

##################################################
10. Configuring an HTPasswd identity provider
##################################################

htpasswd -c -B -b users.htpasswd ocpadmin P@ssw0rd
htpasswd -B -b users.htpasswd ocpmonitor P@ssw0rd
htpasswd -B -b users.htpasswd operation01 P@ssw0rd
htpasswd -B -b users.htpasswd operation02 P@ssw0rd
htpasswd -B -b users.htpasswd operation03 P@ssw0rd
htpasswd -B -b users.htpasswd developer01 P@ssw0rd
htpasswd -B -b users.htpasswd developer02 P@ssw0rd
htpasswd -B -b users.htpasswd developer03 P@ssw0rd

oc create secret generic htpasswd-secret --from-file=htpasswd=</path/to/users.htpasswd> -n openshift-config

vi htpasswd_provider.yaml

apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  name: cluster
spec:
  identityProviders:
  - name: htpasswd
    mappingMethod: claim 
    type: HTPasswd
    htpasswd:
      fileData:
        name: htpasswd-secret

oc apply -f htpasswd_provider.yaml

oc adm policy add-cluster-role-to-user cluster-admin ocpadmin
oc adm policy add-cluster-role-to-user cluster-monitoring-view ocpmonitor
oc adm policy add-cluster-role-to-user cluster-admin operation01
oc adm policy add-cluster-role-to-user cluster-admin operation02
oc adm policy add-cluster-role-to-user cluster-admin operation03

##################################################
11. Disable self provision
##################################################

oc describe clusterrolebinding.rbac self-provisioners
oc patch clusterrolebinding.rbac self-provisioners -p '{"subjects": null}'
oc edit clusterrolebinding.rbac self-provisioners

apiVersion: authorization.openshift.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "false"
  ...
  
oc describe clusterrolebinding.rbac self-provisioners

##################################################
12. Enabling etcd encryption
##################################################

oc edit apiserver

spec:
  encryption:
    type: aescbc

oc get openshiftapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'
oc get kubeapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'

##################################################
13. Installing Red Hat OpenShift Service Mesh
##################################################

13.1 Installing the Elasticsearch Operator

Procedure

    - Log in to the OpenShift Container Platform web console.
    - Navigate to Operators → OperatorHub.
    - Type Elasticsearch into the filter box to locate the Elasticsearch Operator.
    - Click the Elasticsearch Operator to display information about the Operator.
    - Click Install.
    - On the Create Operator Subscription page, select All namespaces on the cluster (default). This installs the Operator in the default openshift-operators project and makes the Operator available to all projects in the cluster.
    - In the Update Channel, select the most current version.
    - Select the Automatic Approval Strategy.
    	
    The Manual approval strategy requires a user with appropriate credentials to approve the Operator install and subscription process.
    
    - Click Subscribe.
    - The Installed Operators page displays the Elasticsearch Operator’s installation progress.
    
oc get csv --all-namespaces|grep -i elasticsearch

13.2 Installing the Jaeger Operator

Procedure

    - Log in to the OpenShift Container Platform web console.
    - Navigate to Operators → OperatorHub.
    - Type Jaeger into the filter box to locate the Jaeger Operator.
    - Click the Jaeger Operator provided by Red Hat to display information about the Operator.
    - Click Install.
    - On the Create Operator Subscription page, select All namespaces on the cluster (default). This installs the Operator in the default openshift-operators project and makes the Operator available to all projects in the cluster.
    - Select the stable Update Channel.
    - Select the Automatic Approval Strategy.
    	
    The Manual approval strategy requires a user with appropriate credentials to approve the Operator install and subscription process.

    - Click Subscribe.
    - The Installed Operators page displays the Jaeger Operator’s installation progress.
 
oc get csv --all-namespaces|grep -i jaeger

13.2 Installing the Kiali Operator

Procedure

    - Log in to the OpenShift Container Platform web console.
    - Navigate to Operators → OperatorHub.
    - Type Kiali into the filter box to find the Kiali Operator.
    - Click the Kiali Operator provided by Red Hat to display information about the Operator.
    - Click Install.
    - On the Create Operator Subscription page, select All namespaces on the cluster (default). This installs the Operator in the default openshift-operators project and makes the Operator available to all projects in the cluster.
    - Select the stable Update Channel.
    - Select the Automatic Approval Strategy.
    	
    The Manual approval strategy requires a user with appropriate credentials to approve the Operator install and subscription process.
    
    - Click Subscribe.
    - The Installed Operators page displays the Kiali Operator’s installation progress.

oc get csv --all-namespaces|grep -i kiali

13.3 Installing the Red Hat OpenShift Service Mesh Operator

Procedure

    - Log in to the OpenShift Container Platform web console.
    - Navigate to Operators → OperatorHub.
    - Type Red Hat OpenShift Service Mesh into the filter box to find the Red Hat OpenShift Service Mesh Operator.
    - Click the Red Hat OpenShift Service Mesh Operator to display information about the Operator.
    - On the Create Operator Subscription page, select All namespaces on the cluster (default). This installs the Operator in the default openshift-operators project and makes the Operator available to all projects in the cluster.
    - Click Install.
    - Select the stable Update Channel.
    - Select the Automatic Approval Strategy.
    	
    The Manual approval strategy requires a user with appropriate credentials to approve the Operator install and subscription process.

    - Click Subscribe.
    - The Installed Operators page displays the Red Hat OpenShift Service Mesh Operator’s installation progress.

oc get csv --all-namespaces|grep -i mesh

13.4 Deploying the Red Hat OpenShift Service Mesh control plane

oc login https://{HOSTNAME}:6443
oc new-project istio-system

vi istio-installation.yaml

apiVersion: maistra.io/v1
kind: ServiceMeshControlPlane
metadata:
  name: basic-install
spec:

  istio:
    global:
      proxy:
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 128Mi

    gateways:
      istio-egressgateway:
        autoscaleEnabled: false
      istio-ingressgateway:
        autoscaleEnabled: false
        ior_enabled: false

    mixer:
      policy:
        autoscaleEnabled: false

      telemetry:
        autoscaleEnabled: false
        resources:
          requests:
            cpu: 100m
            memory: 1G
          limits:
            cpu: 500m
            memory: 4G

    pilot:
      autoscaleEnabled: false
      traceSampling: 100

    kiali:
      enabled: true

    grafana:
      enabled: true

    tracing:
      enabled: true
      jaeger:
        template: all-in-one

oc create -n istio-system -f istio-installation.yaml
oc get smcp -n istio-system
oc get pods -n istio-system -w

Example:

NAME                                     READY   STATUS             RESTARTS   AGE
grafana-7bf5764d9d-2b2f6                 2/2     Running            0          28h
istio-citadel-576b9c5bbd-z84z4           1/1     Running            0          28h
istio-egressgateway-5476bc4656-r4zdv     1/1     Running            0          28h
istio-galley-7d57b47bb7-lqdxv            1/1     Running            0          28h
istio-ingressgateway-dbb8f7f46-ct6n5     1/1     Running            0          28h
istio-pilot-546bf69578-ccg5x             2/2     Running            0          28h
istio-policy-77fd498655-7pvjw            2/2     Running            0          28h
istio-sidecar-injector-df45bd899-ctxdt   1/1     Running            0          28h
istio-telemetry-66f697d6d5-cj28l         2/2     Running            0          28h
jaeger-896945cbc-7lqrr                   2/2     Running            0          11h
kiali-78d9c5b87c-snjzh                   1/1     Running            0          22h
prometheus-6dff867c97-gr2n5              2/2     Running            0          28h

13.5 Creating the Red Hat OpenShift Service Mesh member roll

vi servicemeshmemberroll-default.yaml

apiVersion: maistra.io/v1
kind: ServiceMeshMemberRoll
metadata:
  name: default
  namespace: istio-system
spec:
  members:
    # a list of projects joined into the service mesh
    - <your-project-name>
    - <another-project-name>
    - bookinfo

oc create -n istio-system -f servicemeshmemberroll-default.yaml

13.6 Example Application

oc new-project bookinfo

Add project to Service Mesh Member Roll
oc -n istio-system patch --type='json' smmr default -p '[{"op": "add", "path": "/spec/members", "value":["'"bookinfo"'"]}]'

Deploy booinfo app
oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-1.1/samples/bookinfo/platform/kube/bookinfo.yaml

Create the ingress gateway
oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-1.1/samples/bookinfo/networking/bookinfo-gateway.yaml

export GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath='{.spec.host}')

Add destination rules

If did not enable mutual TLS:
oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-1.1/samples/bookinfo/networking/destination-rule-all.yaml

If enabled mutual TLS:
oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-1.1/samples/bookinfo/networking/destination-rule-all-mtls.yaml

Verifying the bookinfo
curl -o /dev/null -s -w "%{http_code}\n" http://$GATEWAY_URL/productpage
oc get pods -n bookinfo

##################################################
X. Disabling etcd encryption
##################################################

oc edit apiserver

spec:
  encryption:
    type: identity

oc get openshiftapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'
oc get kubeapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'

##################################################
X. OpenShift Custom Console (Custom Link)
##################################################

oc get consolelink

vi console-custom-link.yaml

apiVersion: console.openshift.io/v1
kind: ConsoleLink
metadata:
  name: vayu-console
spec:
  applicationMenu:
    section: Test
  href: https://<URL>/
  location: ApplicationMenu
  text: Example link
  
oc create -f console-custom-link.yaml





