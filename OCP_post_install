##################################################
1. Update NTP and timezone
##################################################

for i in `oc get nodes | grep -iv name | awk '{print $1}'`; do ssh core@$i "sudo timedatectl set-timezone Asia/Bangkok"; done 
for i in `oc get nodes | grep -iv name | awk '{print $1}'`; do echo $i; ssh core@$i "sudo timedatectl"; done 

cat /etc/chrony.conf | python3 -c "import sys, urllib.parse; print(urllib.parse.quote(''.join(sys.stdin.readlines())))"

vi 50-master-chrony
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 50-master-chrony
spec:
  config:
    ignition:
      version: 2.2.0
    storage:
      files:
      - contents:
          source: data:,%23%20Use%20public%23%20Enable%20kernel%20synchronization%20
        filesystem: root
        mode: 0644
        path: /etc/chrony.conf

vi 50-worker-chrony
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 50-master-chrony
spec:
  config:
    ignition:
      version: 2.2.0
    storage:
      files:
      - contents:
          source: data:,%23%20Use%20public%20servers%20from%20the%20pool.ntp.org%20
        filesystem: root
        mode: 0644
        path: /etc/chrony.conf

oc create -f 50-master-chrony
oc create -f 50-worker-chrony
watch oc get mcp
for i in `oc get nodes | grep -iv name | awk '{print $1}'`; do echo $i; ssh core@$i "sudo chronyc sources"; done 

##################################################
2.Create node label
##################################################

oc label node <node> node-role.kubernetes.io/infra-router=""
oc label node <node> node-role.kubernetes.io/infra-monitoring=""

oc label node <node> node-role.kubernetes.io/worker-

oc get nodes --show-labels

##################################################
3. Create infra machineconfigpool
##################################################

cat <<EOF | oc apply -f -
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata: 
  name: infra
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: 
[worker,infra]}
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/infra-router: ""
  paused: false
EOF


cat <<EOF | oc apply -f -
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata: 
  name: monitoring
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: 
[worker,monitoring]}
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/infra-monitoring: ""
  paused: false
EOF

oc get mcp

##################################################
4. Config ImageRegistry Operator to managed state
##################################################

oc edit configs.imageregistry.operator.openshift.io

managementState: Removed -> Managed

  storage:
    emptyDir: {}

##################################################
5. taint node and torelation
##################################################

oc adm taint node <Infra Node1> <Infra Node1> infra-router=reserved:NoSchedule
oc adm taint node <Monitoring Node1> <Monitoring Node2> <Monitoring Node3> infra-monitoring=reserved:NoSchedule

##################################################
6. Move OCP Services to Infra and Monitoring nodes
##################################################

move router pods

oc patch ingresscontroller/default --type=merge -n openshift-ingress-operator -p '{"spec": {"nodePlacement":{"nodeSelector":{"matchLabels":{"node-role.kubernetes.io/infra-router": ""}},"tolerations": [{"effect":"NoSchedule","key": "infra-router","value": "reserved"}]}}}'
watch "oc -n openshift-ingress get pod -o wide"

move image-registry pod

oc -n openshift-image-registry patch configs.imageregistry.operator.openshift.io cluster --type=merge --patch='{"spec":{"nodeSelector": {"node-role.kubernetes.io/infra-monitoring":""},"tolerations": [{"effect":"NoSchedule","key": "infra-monitoring","value": "reserved"}]}}'
watch oc -n openshift-image-registry get pod -o wide | grep '^image-registry-'

##################################################
7. Installing the Local Storage Operator
##################################################

To install the Local Storage Operator from the web console, follow these steps:

    - Log in to the OpenShift Container Platform web console.
    - Navigate to Operators â†’ OperatorHub.
    - Type Local Storage into the filter box to locate the Local Storage Operator.
    - Click Install.
    - On the Create Operator Subscription page, select A specific namespace on the cluster. Select local-storage from the drop-down menu.
    - Adjust the values for Update Channel and Approval Strategy to the values that you want.
    - Click Subscribe.

Once finished, the Local Storage Operator will be listed in the Installed Operators section of the web console.

oc new-project local-storage

oc patch ds local-storage-local-diskmaker -n local-storage -p '{"spec": {"template": {"spec": {"tolerations":[{"operator": "Exists"}]}}}}'
oc patch ds local-storage-local-provisioner -n local-storage -p '{"spec": {"template": {"spec": {"tolerations":[{"operator": "Exists"}]}}}}'

oc get csvs -n local-storage

On Monitoring node1

vgcreate monitoring_vg /dev/sdb
lvcreate -n elasticsearch1_lv -L 300G monitoring_vg
lvcreate -n prometheus1_lv -L 200G monitoring_vg
lvcreate -n alertmanager1_lv -L 5G monitoring_vg

On Monitoring node2

vgcreate monitoring_vg /dev/sdb
lvcreate -n elasticsearch1_lv -L 300G monitoring_vg
lvcreate -n prometheus1_lv -L 200G monitoring_vg
lvcreate -n alertmanager1_lv -L 5G monitoring_vg

On Monitoring node3

vgcreate monitoring_vg /dev/sdb
lvcreate -n elasticsearch1_lv -L 300G monitoring_vg
lvcreate -n alertmanager1_lv -L 5G monitoring_vg


vi local-volume-elasticsearch.yml

apiVersion: "local.storage.openshift.io/v1"
kind: "LocalVolume"
metadata:
  name: "local-storage-elasticsearch"
  namespace: "local-storage"
spec:
  nodeSelector:
    nodeSelectorTerms:
    - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - <Strorage Node1>
          - <Strorage Node2>
          - <Strorage Node3>
  tolerations:
  - operator: Exists
  storageClassDevices:
    - storageClassName: "local-sc-elasticsearch"
      volumeMode: Filesystem
      fsType: xfs
      devicePaths:
        - /dev/monitoring_vg/elasticsearch1_lv

vi local-volume-alertmanager.yml

apiVersion: "local.storage.openshift.io/v1"
kind: "LocalVolume"
metadata:
  name: "local-storage-alertmanager"
  namespace: "local-storage" 
spec:
  nodeSelector: 
    nodeSelectorTerms:
    - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - <Strorage Node1>
          - <Strorage Node2>
          - <Strorage Node3>
  tolerations:
  - operator: Exists
  storageClassDevices:
    - storageClassName: "local-sc-alertmanager"
      volumeMode: Filesystem
      fsType: xfs
      devicePaths: 
        - /dev/monitoring_vg/alertmanager1_lv

vi local-volume-prometheus.yml

apiVersion: "local.storage.openshift.io/v1"
kind: "LocalVolume"
metadata:
  name: "local-storage-prometheus"
  namespace: "local-storage" 
spec:
  nodeSelector: 
    nodeSelectorTerms:
    - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - <Strorage Node1>
          - <Strorage Node2>
  tolerations:
  - operator: Exists
  storageClassDevices:
    - storageClassName: "local-sc-prometheus"
      volumeMode: Filesystem
      fsType: xfs
      devicePaths: 
        - /dev/monitoring_vg/prometheus1_lv

oc create -f local-volume-elasticsearch.yml
oc create -f local-volume-alertmanager.yml
oc create -f local-volume-prometheus.yml








